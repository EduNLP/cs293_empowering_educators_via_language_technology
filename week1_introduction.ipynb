{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQx7mN9-tsJ3"
      },
      "source": [
        "# Week 1: Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et5JBI06tmQ0"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Rose E. Wang, Dorottya Demszky\"\n",
        "__version__ = \"CS293/EDUC473, Stanford, Fall 2023\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WYTDwb06w0fM"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "* [Overview](#overview)\n",
        "    * [Deliverables](#your-deliverables)\n",
        "    * [Linking Colab Notebooks to your GDrive](#linking-this-colab-to-your-gdrive)\n",
        "* [Downloading the NCTE dataset](#downloading-the-dataset)\n",
        "* [Exploring the NCTE dataset](#exploring-the-ncte-dataset)\n",
        "    * [Transcript Data](#exploring-the-transcripts)\n",
        "    * [Metadata](#exploring-the-metadata)\n",
        "    * [Student Reasoning Data](#exploring-the-student-reasoning-data)\n",
        "* [Text Analysis Tools](#text-analysis-tools)\n",
        "    * [Word Frequencies](#word-frequencies)\n",
        "    * [Log-Odds Analysis](#log-odds-analysis)\n",
        "    * [Topic-Modelling](#topic-modeling)\n",
        "    * [Clustering](#clustering)\n",
        "* [Assignment](#assignments)\n",
        "* [Extra Assignment](#extra-assignments)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyoQ3BI8d07"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The purpose of this notebook is to introduce to you the dataset we'll be working with throughout the quarter and perform some simple data analysis on the dataset.\n",
        "\n",
        "\n",
        "The dataset we'll be using this quarter is called the [NCTE dataset](https://arxiv.org/pdf/2211.11772.pdf) [1], which contains classroom transcripts of 4th and 5th grade mathematics classes.\n",
        "The dataset was collected by the National Center for Teacher Effectivenesss (NCTE) between 2010 and 2013, and it contains anonymized transcripts represent data from 317 teachers across 4 school districts that serve largely historically marginalized students. The transcripts come with rich metadata, including turn-level annotations for dialogic discourse moves, classroom observation scores, demographic information, survey responses and student test scores. For more details on the dataset, please refer to the aforementioned reference.\n",
        "\n",
        "[1] Demszky, D., & Hill, H. (2022). The NCTE transcripts: A dataset of elementary math classroom transcripts. arXiv preprint arXiv:2211.11772.\n",
        "\n",
        "### Your Deliverables\n",
        "\n",
        "To receive credit for this assignment, please upload the PDF version of your ENTIRE Colab that includes all your code and written responses to Gradescope.\n",
        "\n",
        "### Note\n",
        "\n",
        "**We are assuming you are running everything within [Colab](https://colab.research.google.com/).** We are not responsible for bugs created outside of the Colab environment.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nXaZYAIcw_5q"
      },
      "source": [
        "## Linking this Colab to your GDrive\n",
        "\n",
        "Please download the repository from [https://github.com/rosewang2008/cs293_empowering_educators_via_language_technology](https://github.com/rosewang2008/cs293_empowering_educators_via_language_technology).\n",
        "\n",
        "Then, upload the repository to your Google Drive and link the directory to all future colabs. The following command should link your GDrive to this colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHr8cpLIxJOQ",
        "outputId": "5c41fae8-f723-4420-bd2d-f6f769ea0717"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S2wawFf1xnQ9"
      },
      "source": [
        "Specify the directory path to where you've uploaded the GitHub repository.\n",
        "Within the directory path you've specified, it should contain things like the `data` folder or python notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj9vJo8txUaF"
      },
      "outputs": [],
      "source": [
        "DIR_PATH = \"/content/drive/your_path_to_the_folder_REPLACE_ME/\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Downloading the NCTE dataset\n",
        "\n",
        "1. Fill out the Google form in [https://github.com/ddemszky/classroom-transcript-analysis](https://github.com/ddemszky/classroom-transcript-analysis).\n",
        "2. That form sould share with you a Google Drive folder with the files: `ncte_single_utterances.csv`, `student_reasoning.csv` and `paired_annotations.csv`.\n",
        "3. Download these file and put it in your Google Drive folder for the course, e.g., if `DIR_PATH = /content/drive/empowering_educators_via_language_technology/` then you should put the CSV files in the existing `data` folder: `/content/drive/empowering_educators_via_language_technology/data/`\n",
        "\n",
        "The final output should be:\n",
        "```\n",
        "/content/drive/empowering_educators_via_language_technology/data/\n",
        "    └── ncte_csingle_utterances.csv\n",
        "    └── student_reasoning.csv \n",
        "    └── paired_annotations.csv \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX-2VR6Fw0fO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Pretty plotting\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6kBGfTDRw0fS"
      },
      "source": [
        "## Exploring the NCTE dataset\n",
        "\n",
        "### Exploring the transcripts\n",
        "Let's explore the NCTE transcripts, `ncte_single_utterances.csv`, which contains all the utterances from the transcript dataset.\n",
        "The `OBSID` column represents the unique ID for the transcript, and the `NCTETID` represents the teacher ID, which are mappable to metadata.\n",
        "`comb_idx` represents a unique ID for each utterance (concatenation of OBSID and turn_idx), which is mappable to turn-level annotations.\n",
        "\n",
        "The other two files `paired_annotations.csv` and `student_reasoning.csv` are derivatives of the utterance csv.\n",
        "We will explore the `student_reasoning.csv` file later.\n",
        "\n",
        "Now, let's take a closer look at `single_utterances.csv` and perform some basic checks of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAuHxA7atryN"
      },
      "outputs": [],
      "source": [
        "utterances = pd.read_csv(os.path.join(DIR_PATH,'data/ncte_single_utterances.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "O6wuh4xjMGgN",
        "outputId": "28c170f2-54d1-4b4c-8a1f-89dfc1ad3db7"
      },
      "outputs": [],
      "source": [
        "print(f\"Columns: {utterances.columns}\")\n",
        "\n",
        "utterances"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-SVgdd1mw0fU"
      },
      "source": [
        "The columns in `utterances` are:\n",
        "- `speaker`: The speaker of the utterance.\n",
        "- `text`: The utterance text.\n",
        "- `year`: The school year in which transcript was taken. 1 = 2010-11, 2 = 2011-12, 3 = 2012-13 school year.\n",
        "- `OBSID`: The unique ID for the transcript.\n",
        "- `video_id`: The unique ID of the video from which the transcript was taken.\n",
        "- `cleaned_text`: The cleaned version of `text` with removed punctuation and lower casing.\n",
        "- `num_words`: Number of words in the utterance text.\n",
        "- `turn_idx`: The utterance turn number in the transcript.\n",
        "- `comb_idx`: The concatenation of `OBSID` and `turn_idx`, i.e., `comb_idx = <OBSID>_<turn_idx>`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "otH1LlGmw0fV"
      },
      "source": [
        "Let's take a look at an example of a classroom transcript. We'll pick one of the transcript IDs and print the transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF4g5z8Nw0fV"
      },
      "outputs": [],
      "source": [
        "# Get the unique transcript IDs, sort, and pick the first ID to print transcript for.\n",
        "transcript_id = sorted(list(utterances['OBSID'].unique()))[0] # 3\n",
        "transcript_df = utterances[utterances['OBSID'] == transcript_id]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6E7s4DkGw0fa"
      },
      "source": [
        "This code will take a df that contains all the utterances from the same classroom session, sort the utterances by a column indicating utterance order, and print each utterance as: `[speaker_name]: [utterance]\\n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnwC6yzWw0fb"
      },
      "outputs": [],
      "source": [
        "def construct_transcript_from_df(transcript_df, sort_by, speaker_column, text_column, print_transcript=False):\n",
        "    transcript_text = \"\"\n",
        "    # Sort the transcript by the sort_by column\n",
        "    transcript_df = transcript_df.sort_values(by=sort_by)\n",
        "    # Add to the transcript_text as `speaker: text` for each row in the dataframe\n",
        "    for index, row in transcript_df.iterrows():\n",
        "        transcript_text += row[speaker_column] + \": \" + row[text_column] + \"\\n\"\n",
        "    # Print the transcript text if print_transcript is True\n",
        "    if print_transcript:\n",
        "        print(transcript_text)\n",
        "    return transcript_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iZxES8Nw0fb",
        "outputId": "3873eae4-2d0a-44bd-8e98-c083607b96be"
      },
      "outputs": [],
      "source": [
        "transcript_text = construct_transcript_from_df(\n",
        "    transcript_df=transcript_df,\n",
        "    sort_by='turn_idx',\n",
        "    speaker_column='speaker',\n",
        "    text_column='text',\n",
        "    print_transcript=False\n",
        ")\n",
        "\n",
        "print(f\"Transcript:\\n{transcript_text}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qQzAQqI2w0fi"
      },
      "source": [
        "Before performing any analysis on the transcript _text_, we perform some analysis based on the column information already provided in the dataframe!\n",
        "This is good practice to get a sense of numbers and scale in a new dataset.\n",
        "\n",
        "For example, we can look at the **number of total transcripts, and number of transcripts per school year**..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "3jWbVMLhw0fi",
        "outputId": "e5b3cc26-937b-42ca-a6ad-2cb6292196f5"
      },
      "outputs": [],
      "source": [
        "# Count total number of transcripts\n",
        "total_transcripts = len(utterances['OBSID'].unique())\n",
        "print(f\"Total number of transcripts: {total_transcripts}\")\n",
        "# Count number of unique `OBSID`s per `year`\n",
        "transcripts_per_year = utterances.groupby('year')['OBSID'].nunique()\n",
        "print(f\"Transcripts per year:\\n{transcripts_per_year}\")\n",
        "\n",
        "# Plot transcripts per year\n",
        "sns.barplot(x=transcripts_per_year.index, y=transcripts_per_year.values)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of transcripts')\n",
        "plt.title('Number of transcripts per year')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RDHdbFkvw0fj"
      },
      "source": [
        "We can also take a look at some statistics regarding the transcript utterances.\n",
        "\n",
        "For example, we can look at the **average number of utterances per transcript**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxEOpVtJw0fj",
        "outputId": "69b5b123-f011-4bfc-be9e-4d883851b87f"
      },
      "outputs": [],
      "source": [
        "# Average number of utterance turns `turn_idx` across transcripts `OBSID`\n",
        "avg_turns = utterances.groupby('OBSID')['turn_idx'].max().mean()\n",
        "print(f\"Average number of utterance turns across transcripts: {avg_turns:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5VZWoauuw0fm"
      },
      "source": [
        "We might also be interested in the average number of utterances spoken by the teacher vs. the students in the classroom:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBTtoUNw0fu",
        "outputId": "909248a8-9b23-476f-8741-0ea05837e891"
      },
      "outputs": [],
      "source": [
        "# Average number of utterance turns `turn_idx` per speaker `speaker` across all transcripts `OBSID`:\n",
        "unique_speakers = utterances['speaker'].unique()\n",
        "for speaker in unique_speakers:\n",
        "    if pd.isnull(speaker):\n",
        "        continue\n",
        "    speaker_df = utterances[utterances['speaker'] == speaker]\n",
        "    # Count number of turns per transcript\n",
        "    turns_df = speaker_df.set_index('OBSID')['turn_idx'].value_counts()\n",
        "    # Average the number of turns across transcripts\n",
        "    avg_turns_per_speaker = turns_df.mean()\n",
        "    print(f\"Average # turns for speaker {speaker}: {avg_turns_per_speaker}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TUe8Zp0Ow0fv"
      },
      "source": [
        "Finally, let's look at the **number of words each speaker uses on average on their utterance turns**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlaGd8wow0fv",
        "outputId": "3d86726d-34e0-4ecf-8f1e-ae3431df24b4"
      },
      "outputs": [],
      "source": [
        "# Average number of words per speaker\n",
        "for speaker in unique_speakers:\n",
        "    if pd.isnull(speaker):\n",
        "        continue\n",
        "    speaker_df = utterances[utterances['speaker'] == speaker]\n",
        "    avg_words = speaker_df['num_words'].mean()\n",
        "    print(f\"Average # words for {speaker}: {avg_words:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjNRYyOw0fw"
      },
      "source": [
        "**Things to think about**:\n",
        "- The \"average # turns\" cell shows that the `teacher` and `student`s have similar number of turns across transcripts, i.e., ~239 teacher utterances compared to ~211 student utterances.\n",
        "- However, the average # of words indicates a noticeable difference in their utterance lengths, i.e., ~29 words in the teachers' utterances compared to ~4 words in the students' utterances."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j9rVO19aw0fw"
      },
      "source": [
        "#### Manually checking the (quality of the) text\n",
        "\n",
        "Before even running any kind of \"smart\" analysis methods on the text, it's always good to manually look at examples.\n",
        "Previously we printed out an example of a transcript.\n",
        "Here, we are going to print out a few examples of the teacher utterances and student utterances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHRiSONVw0fx"
      },
      "outputs": [],
      "source": [
        "# Helpful utility function!\n",
        "def print_line_separated(df, speaker_column, text_column):\n",
        "    for speaker, text in zip(df[speaker_column], df[text_column]):\n",
        "        print(f\"{speaker}: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl2nDNFjw0fy",
        "outputId": "2172aef8-b8c4-4916-d983-aecc5848d56a"
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 15\n",
        "# Seed the random number generator to get the same results\n",
        "np.random.seed(42)\n",
        "\n",
        "# Get NUM_SAMPLES of the teacher utterances\n",
        "teacher_df = utterances[utterances['speaker'] == 'teacher'].sample(NUM_SAMPLES)\n",
        "print(\"Teacher utterances:\")\n",
        "print_line_separated(df=teacher_df, speaker_column='speaker', text_column='text')\n",
        "print()\n",
        "\n",
        "# Get NUM_SAMPLES of the student utterances\n",
        "student_df = utterances[utterances['speaker'] == 'student'].sample(NUM_SAMPLES)\n",
        "print(\"Student utterances:\")\n",
        "print_line_separated(df=student_df, speaker_column='speaker', text_column='text')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snXM3FVmw0fy",
        "outputId": "ea0632d6-9813-48e1-f3a3-adfaaa003b96"
      },
      "outputs": [],
      "source": [
        "\"\"\">>>Expected result:\n",
        "\n",
        "Teacher utterances:\n",
        "teacher: Okay, I can double it by doing 15 times 4?  Does everybody agree with that?  So this is doubling it?\n",
        "teacher: Okay, go get paper, pencils.  This group, you're gonna work over here together.\n",
        "teacher: How they've shown it on here, every part of what they did they're able to show on here and you're able to see, right. That's the way it needs to be when you're showing your work. Let's see.  Student A, come here.  You can come up.  Can you tell us how this side works?\n",
        "teacher: Because that’s not really modeling the tenth.  It’s all got to be [inaudible].  Nope.\n",
        "teacher: Student B's answering.\n",
        "teacher: All your pages are filled up?\n",
        "teacher: I love what you just said.  Did everyone hear that?\n",
        "teacher: Keep going.\n",
        "teacher: Add a 0.  Now let's look at the numbers.  The percent – does that make sense?  Are these two numbers pretty close to each other?\n",
        "teacher: To ten, right?\n",
        "teacher: That’s a good strategy.  So you’re imagining – one second. You’re imagining the two zeros being in place if it’s a tenth [Inaudible]?  Good.  Make sure you share that out later on.\n",
        "teacher: Huh?\n",
        "teacher: Okay.  Boards down.  We’re going to try again because there was some noise.\n",
        "teacher: We’re going to measure this height, right?  So we’re doing L, right?\n",
        "teacher: Oh, you’re fine, you can keep writing because I’ve got to do this again.  It didn't like me before.  How many people are still writing?  Okay.\n",
        "\n",
        "Student utterances:\n",
        "student: They’re all wearing two –\n",
        "student: I know the answer for [inaudible].\n",
        "student: So we’re making a story problem like the other one?\n",
        "student: Mrs. H, can you put this inside the bag?\n",
        "student: You divide nine fifteenths.\n",
        "student: Yes.\n",
        "student: Okay, you get like [inaudible].\n",
        "student: A part of a whole.\n",
        "student: Never mind.\n",
        "student: Subtract.\n",
        "student: A triangle, a star, a circle or a square.\n",
        "student: First I did this.  [Inaudible].\n",
        "student: Improper fraction.\n",
        "student: 14.\n",
        "student: Eight thousand.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rtriKWstw0fz"
      },
      "source": [
        "It's always good to make observations about the type of texts or patterns in the data.\n",
        "\n",
        "**Things to think about:** Manually looking at the _teacher_ utterances, we notice a few things.\n",
        "- Some of the utterances are math content related (e.g., \"Add a 0.  Now let's look at the numbers.  The percent – does that make sense?  Are these two numbers pretty close to each other?\") and others are not (e.g., \"Okay, go get paper, pencils.  This group, you're gonna work over here together.\")\n",
        "- Some of the utterances are about class management such as calling everyone's attention, \"I love what you just said.  Did everyone hear that?\"\n",
        "- Some of the utterances are about supporting the student's thinking e.g., \"Okay, I can double it by doing 15 times 4?  Does everybody agree with that?  So this is doubling it?\"\n",
        "\n",
        "\n",
        "**Things to think about:** Manually looking at the _student_ utterances, we notice a few things.\n",
        "- The utterances are shorter than the teacher's utterances.\n",
        "- The utterances seem to be short answers to questions e.g., \"14.\", \"Yes.\", \"Subtract.\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nZxwebygw0fz"
      },
      "source": [
        "### Exploring the metadata\n",
        "\n",
        "Now let's switch gears to explore the metadata folder `data/ICPSR_36095`!\n",
        "This folder contains many subfolders `DS00##`, each of which contains a `tsv` file and PDF documenting the contents of the `tsv` file.\n",
        "Each subfolder contains different types of metadata.\n",
        "For example, `DS0001` contains the metadata and documentation for class observations, a rubric on the teacher's class management and behavior.\n",
        "Or, `DS0006` contains metadata and documentation on a teacher background questionnaire.\n",
        "\n",
        "\n",
        "In this section we're going to use `DS0006` and compute the general statistics on the teacher's questionnaire metadata.\n",
        "\n",
        "In particular, we are going to determine:\n",
        "- Number of teachers\n",
        "- % Male\n",
        "- % Black\n",
        "- % Asian\n",
        "- % Hispanic\n",
        "- % White\n",
        "- Avg number of years of teaching experience\n",
        "- % BA in education?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "AQ9kI6h7w0fz",
        "outputId": "cdb4aa95-e7b0-4d2e-cf87-93dcc8d5d114"
      },
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "fpath = os.path.join(DIR_PATH,'data/ICPSR_36095/DS0006/36095-0006-Data.tsv')\n",
        "teacher_metadata = pd.read_csv(fpath, sep='\\t')\n",
        "print(teacher_metadata.columns)\n",
        "teacher_metadata.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RtTAxoivw0fz"
      },
      "source": [
        "Using the documentation `data/ICPSR_36095/DS0006/36095-0006-Codebook.pdf`, we can easily compute the desired statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BVkXpWzw0fz",
        "outputId": "e410ca07-36e5-4781-ec70-4f0987571e33"
      },
      "outputs": [],
      "source": [
        "# Number of teachers\n",
        "num_teachers = teacher_metadata['NCTETID'].nunique()\n",
        "print(f\"Number of teachers = {num_teachers}\")\n",
        "\n",
        "# Percentage Male\n",
        "# Drop non 0/1 values\n",
        "teacher_metadata = teacher_metadata[teacher_metadata['MALE'].isin([0,1])]\n",
        "num_male = teacher_metadata['MALE'].sum()\n",
        "perc_male = ( num_male / num_teachers ) * 100\n",
        "# Round\n",
        "perc_male = round(perc_male, 2)\n",
        "print(f\"Percentage male = {perc_male}\")\n",
        "\n",
        "# Percentage Black\n",
        "num_black = teacher_metadata['BLACK'].sum()\n",
        "perc_black = ( num_black / num_teachers ) * 100\n",
        "perc_black = round(perc_black, 2)\n",
        "print(f\"Percentage black = {perc_black}\")\n",
        "\n",
        "# Percentage Asian\n",
        "num_asian = teacher_metadata['ASIAN'].sum()\n",
        "perc_asian = ( num_asian / num_teachers ) * 100\n",
        "perc_asian = round(perc_asian, 2)\n",
        "print(f\"Percentage asian = {perc_asian}\")\n",
        "\n",
        "# Percentage Hispanic\n",
        "num_hispanic = teacher_metadata['HISP'].sum()\n",
        "perc_hispanic = ( num_hispanic / num_teachers ) * 100\n",
        "perc_hispanic = round(perc_hispanic, 2)\n",
        "print(f\"Percentage hispanic = {perc_hispanic}\")\n",
        "\n",
        "# Percentage White\n",
        "num_white = teacher_metadata['WHITE'].sum()\n",
        "perc_white = ( num_white / num_teachers ) * 100\n",
        "perc_white = round(perc_white, 2)\n",
        "print(f\"Percentage white = {perc_white}\")\n",
        "\n",
        "# Average number of years of teaching experience\n",
        "# Drop empty strings and ' '\n",
        "teacher_metadata = teacher_metadata[teacher_metadata['EXPERIENCE'] != ' ']\n",
        "# Make sure it's a float\n",
        "teacher_metadata['EXPERIENCE'] = teacher_metadata['EXPERIENCE'].astype(float)\n",
        "avg_years = teacher_metadata['EXPERIENCE'].mean()\n",
        "avg_years = round(avg_years, 2)\n",
        "print(f\"Average number of years of teaching experience = {avg_years}\")\n",
        "\n",
        "# BA in education\n",
        "num_ba = teacher_metadata['EDBACHELORS'].sum()\n",
        "perc_ba = ( num_ba / num_teachers ) * 100\n",
        "perc_ba = round(perc_ba, 2)\n",
        "print(f\"Percentage with BA in education = {perc_ba}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mrgVpyg0w0fz"
      },
      "source": [
        "### Exploring the student reasoning data\n",
        "\n",
        "Finally, we explore the student reasoning data `data/student_reasoning.csv`.\n",
        "The file contains utterances under the column `text` labeled for whether there is student reasoning under the column `student_reasoning`.\n",
        "\n",
        "The data is labeled by humans who saw the context for the utterance (the preceding utterance) and labeled whether the utterance is the student expressing their reasoning.\n",
        "\n",
        "Again, we might want to perform some preliminary checks of the data. In this example, we will look at the number of utterances labeled as student reasoning (or not), and the length of these utterances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "D4ek4kJMw0f0",
        "outputId": "7d1fc1c2-496b-420b-8e82-6bbdd5d0818f"
      },
      "outputs": [],
      "source": [
        "student_reasoning_fpath = os.path.join(DIR_PATH,'data/student_reasoning.csv')\n",
        "student_reasoning = pd.read_csv(student_reasoning_fpath)\n",
        "\n",
        "# Plot the number of utterances labeled as `reasoning`.\n",
        "sns.barplot(x=student_reasoning['student_reasoning'].value_counts().index, y=student_reasoning['student_reasoning'].value_counts().values)\n",
        "plt.xlabel('Is Student Reasoning?')\n",
        "plt.ylabel('Number of utterances')\n",
        "plt.title('Number of utterances labeled as `reasoning`')\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YKVodpfzw0f0"
      },
      "source": [
        "This dataset is imbalanced---meaning that there are more instances of a particular class than of other classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "9vU5Q_BLw0f0",
        "outputId": "61dc38be-c753-468b-9b37-64ad9abfa3a1"
      },
      "outputs": [],
      "source": [
        "# Calculate the length of `text` and plot the average length of `text` per `student_reasoning`\n",
        "# We define length as the number of words in `text`\n",
        "student_reasoning['text_len'] = student_reasoning['text'].apply(lambda x: len(x.split(' ')))\n",
        "sns.barplot(x=student_reasoning['student_reasoning'], y=student_reasoning['text_len'])\n",
        "plt.xlabel('Is Student Reasoning?')\n",
        "plt.ylabel('Average length of text')\n",
        "plt.title('Average length of text per `student_reasoning`')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5K7OKwWMw0f0"
      },
      "source": [
        "Generally, utterances that are marked as `student_reasoning` are longer than ones that are not student reasoning.\n",
        "\n",
        "We can take a look at a few examples as well to better understand the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNAn5cpXw0f0",
        "outputId": "5d93014b-8a6a-474f-80a9-70139683670b"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES = 10\n",
        "\n",
        "# Get NUM_EXAMPLES of the student utterances labeled as `reasoning`\n",
        "reasoning_df = student_reasoning[student_reasoning['student_reasoning'] == 1].sample(NUM_EXAMPLES)\n",
        "\n",
        "# Get NUM_EXAMPLES of the student utterances labeled as `not_reasoning`\n",
        "not_reasoning_df = student_reasoning[student_reasoning['student_reasoning'] == 0].sample(NUM_EXAMPLES)\n",
        "\n",
        "print(\"Reasoning utterances:\")\n",
        "print_line_separated(df=reasoning_df, speaker_column='comb_idx', text_column='text')\n",
        "\n",
        "print(\"\\nNot reasoning utterances:\")\n",
        "print_line_separated(df=not_reasoning_df, speaker_column='comb_idx', text_column='text')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1_DOesnfw0f1"
      },
      "source": [
        "## Text Analysis Tools"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HbAsiDCnw0f1"
      },
      "source": [
        "### Word Frequencies\n",
        "\n",
        "This section will perform some simple text analysis on the transcripts from above.\n",
        "Previously, we manually checked the quality of the teacher and student utterances. Now, let's look at what kinds of words the students are saying compared to the teachers.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnbr4hm-w0f1"
      },
      "source": [
        "As a simple analysis, we're going to look at **high-frequency words** for the teachers and students.\n",
        "To do this, we're going to be using a library called [NLTK](https://www.nltk.org/).\n",
        "This is a really helpful library for doing basic analysis, and we'll be returning to it again in the Assignments.\n",
        "\n",
        "\n",
        "FIrst, let's install NLTK and download the appropriate modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrNBioUOw0f1",
        "outputId": "efc1d060-18a8-49a0-9862-bef632793b8d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# For tokenizing and tagging\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0RFrsn6w0f2"
      },
      "outputs": [],
      "source": [
        "# We're going to downsample the data to make it easier to work with\n",
        "NUM_SAMPLES=100\n",
        "teacher_df = utterances[utterances['speaker'] == 'teacher'].sample(NUM_SAMPLES)\n",
        "student_df = utterances[utterances['speaker'] == 'student'].sample(NUM_SAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_nZ0ZtTw0f2",
        "outputId": "55da45dc-6ed4-4f1e-d65d-2ffa6fc6f5b3"
      },
      "outputs": [],
      "source": [
        "# \\n join the teacher utterances, and tokenize the utterances on word level\n",
        "joined_teacher_utterances = '\\n'.join(teacher_df['text'].tolist())\n",
        "teacher_tokens = nltk.word_tokenize(joined_teacher_utterances)\n",
        "# Case-insensitive frequency distribution of the teacher utterances\n",
        "teacher_dist = nltk.FreqDist([w.lower() for w in teacher_tokens])\n",
        "# Print the 10 most frequent words\n",
        "print(teacher_dist.most_common(10))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PaGjQ-iLw0f2"
      },
      "source": [
        "Note, how the word-level frequency doesn't seem to be showing anything interesting or math-content related, as we had seen in the examples.\n",
        "There's a lot of punctuation symbols showing up and reference words like 'that' or 'this'.\n",
        "\n",
        "To filter out these common, but less interesting tokens, we can apply some basic filters.\n",
        "One is a filter that removes things that are non-alphabetic characters like punctuation or numbers.\n",
        "The other is a filter that removes stopwords, which are commonly used words like \"the\" \"a\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7fEb6Exw0f2",
        "outputId": "df58824c-15a9-499a-be26-534eddf16a2e"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# w.isalpha() returns True if all characters in w are alphabetic\n",
        "teacher_dist = nltk.FreqDist([\n",
        "    w.lower() for w in teacher_tokens\n",
        "    if w.isalpha() # remove non-alphabetic characters\n",
        "    and w.lower() not in stopwords # remove stopwords e.g., the, a, an, in, of, etc.\n",
        "])\n",
        "# Print the 10 most frequent words\n",
        "print(teacher_dist.most_common(10))\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OZty07pOw0f2"
      },
      "source": [
        "#### Log-Odds Analysis\n",
        "\n",
        "Going beyond just counting frequent words in the student and teacher's utterances, we might be interested in the chances of a word occurring in the student's text over it occurring in the teacher's text.\n",
        "\n",
        "This gets us to a [log-odds analysis](https://en.wikipedia.org/wiki/Odds_ratio) on the words."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EDSj4Iomw0f2"
      },
      "source": [
        "Let's first define some utility functions to calculate the log odds of the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFDeEhEXw0f3"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "def get_counts(tweets, vocab):\n",
        "    counts = {w: 0 for w in vocab}\n",
        "    for split in tweets:\n",
        "        count = 0\n",
        "        prev = ''\n",
        "        for w in split:\n",
        "            if w == '':\n",
        "                continue\n",
        "            if w in vocab:\n",
        "                counts[w] += 1\n",
        "            if count > 0:\n",
        "                bigram = prev + ' ' + w\n",
        "                if bigram in vocab:\n",
        "                    counts[bigram] += 1\n",
        "            count += 1\n",
        "            prev = w\n",
        "    return counts\n",
        "\n",
        "def log_odds(counts1, counts2, prior, zscore = True):\n",
        "    # code from Dan Jurafsky\n",
        "    # note: counts1 will be positive and counts2 will be negative\n",
        "\n",
        "    sigmasquared = defaultdict(float)\n",
        "    sigma = defaultdict(float)\n",
        "    delta = defaultdict(float)\n",
        "\n",
        "    n1 = sum(counts1.values())\n",
        "    n2 = sum(counts2.values())\n",
        "\n",
        "    # since we use the sum of counts from the two groups as a prior, this is equivalent to a simple log odds ratio\n",
        "    nprior = sum(prior.values())\n",
        "    for word in prior.keys():\n",
        "        if prior[word] == 0:\n",
        "            delta[word] = 0\n",
        "            continue\n",
        "        l1 = float(counts1[word] + prior[word]) / (( n1 + nprior ) - (counts1[word] + prior[word]))\n",
        "        l2 = float(counts2[word] + prior[word]) / (( n2 + nprior ) - (counts2[word] + prior[word]))\n",
        "        sigmasquared[word] = 1/(float(counts1[word]) + float(prior[word])) + 1/(float(counts2[word]) + float(prior[word]))\n",
        "        sigma[word] = math.sqrt(sigmasquared[word])\n",
        "        delta[word] = (math.log(l1) - math.log(l2))\n",
        "        if zscore:\n",
        "            delta[word] /= sigma[word]\n",
        "    return delta\n",
        "\n",
        "\n",
        "def get_log_odds_values(group1_df, group2_df, text_column, words2idx):\n",
        "    # get counts\n",
        "    counts1 = get_counts(group1_df[text_column], words2idx)\n",
        "    counts2 = get_counts(group2_df[text_column], words2idx)\n",
        "    prior = {}\n",
        "    for k, v in counts1.items():\n",
        "        prior[k] = v + counts2[k]\n",
        "\n",
        "    # get log odds\n",
        "    # note: we don't z-score because that makes the absolute values for large events significantly smaller than for smaller\n",
        "    # events. however, z-scoring doesn't make a difference for our results, since we simply look at whether the log odds\n",
        "    # are negative or positive (rather than their absolute value)\n",
        "    delta = log_odds(counts1, counts2, prior, True)\n",
        "    return prior, counts1, counts2, delta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GJG15Tr10nm",
        "outputId": "4eb0fa6c-2c6c-4ec6-fbc0-a0e6cbdb834f"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import json\n",
        "\n",
        "# Download nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "punct_chars = list((set(string.punctuation) | {'’', '‘', '–', '—', '~', '|', '“', '”', '…', \"'\", \"`\", '_'}) - set(['#']))\n",
        "punct_chars.sort()\n",
        "punctuation = ''.join(punct_chars)\n",
        "replace = re.compile('[%s]' % re.escape(punctuation))\n",
        "\n",
        "INPUT_DIR = os.path.join(DIR_PATH, \"data/text_processing/\")\n",
        "\n",
        "stopwords = set(open(INPUT_DIR + 'stopwords.txt', 'r').read().splitlines())\n",
        "\n",
        "def clean_text_to_words(text, keep_stopwords, stem):\n",
        "    if not keep_stopwords:\n",
        "        stop = stopwords\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    # eliminate urls\n",
        "    text = re.sub(r'http\\S*|\\S*\\.com\\S*|\\S*www\\S*', ' ', text)\n",
        "    # eliminate @mentions\n",
        "    text = re.sub(r'\\s@\\S+', ' ', text)\n",
        "    # substitute all other punctuation with whitespace\n",
        "    text = replace.sub(' ', text)\n",
        "    # replace all whitespace with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # strip off spaces on either end\n",
        "    text = text.strip()\n",
        "    # stem words\n",
        "    words = text.split()\n",
        "    if not keep_stopwords:\n",
        "        words = [w for w in words if w not in stop]\n",
        "    if stem:\n",
        "        words = [sno.stem(w) for w in words]\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbM1hgzgw0f3"
      },
      "outputs": [],
      "source": [
        "# Let's first clean up the text data a bit more. You can check out how we process the data in utils/text_processing.py.\n",
        "# The functions we used are essentially ones we've already seen in prior steps, but we've added a few more steps to clean up the text data. We encourage you to look at the documentation for new functions you haven't seen before!\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "CLEAN_TEXT_COLUMN = 'cleaned_text'\n",
        "TEXT_COLUMN = 'text'\n",
        "\n",
        "def plot_log_odds(group1, group2, logodds_factor=1.5):\n",
        "  # Let's build a dictionary that maps all words from teacher and student to unique IDs\n",
        "  words = set(teacher_df[CLEAN_TEXT_COLUMN].sum() + student_df[CLEAN_TEXT_COLUMN].sum())\n",
        "  words2idx = {w: i for i, w in enumerate(words)}\n",
        "\n",
        "  # Take a look at the first 10 words in the dictionary\n",
        "  print(list(words2idx.items())[:10])\n",
        "\n",
        "  # Run log odds\n",
        "  _, _, _, log_odds = get_log_odds_values(\n",
        "    group1_df=group1,\n",
        "    group2_df=group2,\n",
        "    text_column=CLEAN_TEXT_COLUMN,\n",
        "    words2idx=words2idx\n",
        "  )\n",
        "\n",
        "  # Show a few of the log odds values\n",
        "  print(list(log_odds.items())[:10])\n",
        "\n",
        "  # Let's create a dataframe with the log odds values, and then plot the top and bottom 10 words in a barplot.\n",
        "  log_odds_df = pd.DataFrame.from_dict(log_odds, orient='index', columns=['log_odds'])\n",
        "  log_odds_df = log_odds_df.sort_values(by='log_odds', ascending=False)\n",
        "  # Plot the words factor*std above and below 0.\n",
        "  mean = 0\n",
        "  std = log_odds_df['log_odds'].std()\n",
        "  factor = logodds_factor\n",
        "  top_bottom_df = pd.concat([log_odds_df[log_odds_df['log_odds'] >= mean + factor * std], log_odds_df[log_odds_df['log_odds'] <= mean - factor * std]])\n",
        "  # x-axis is log odds, y-axis is words\n",
        "  plt.figure(figsize=(8, 15))\n",
        "  sns.barplot(x=top_bottom_df['log_odds'], y=top_bottom_df.index)\n",
        "  plt.xlabel('Log odds')\n",
        "  plt.ylabel('Words')\n",
        "\n",
        "  # Put text on the left and right of the x-axis (more likely to be teacher or student)\n",
        "  x_min, x_max = plt.xlim()\n",
        "  y_min, y_max = plt.ylim()\n",
        "  plt.text(x_min, y_min, 'More likely to be student', ha='left', va='center')\n",
        "  plt.text(x_max, y_min, 'More likely to be teacher', ha='right', va='center')\n",
        "\n",
        "  plt.title('Words by log odds')\n",
        "  plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UPOSL00Q40vK"
      },
      "source": [
        "Let's look at the log-odds analysis where we keep the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NvaCUqEPw0f3",
        "outputId": "43cbcc3e-e465-4602-96e9-eb8606b58b42"
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES = 100\n",
        "teacher_df = utterances[utterances['speaker'] == 'teacher'].sample(NUM_SAMPLES)\n",
        "student_df = utterances[utterances['speaker'] == 'student'].sample(NUM_SAMPLES)\n",
        "teacher_df[CLEAN_TEXT_COLUMN] = teacher_df[TEXT_COLUMN].apply(lambda x: clean_text_to_words(x, keep_stopwords=True, stem=False))\n",
        "student_df[CLEAN_TEXT_COLUMN] = student_df[TEXT_COLUMN].apply(lambda x: clean_text_to_words(x, keep_stopwords=True, stem=False))\n",
        "plot_log_odds(teacher_df, student_df, logodds_factor=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kN8wIR3745CH"
      },
      "source": [
        "Now, let's compare it to when we don't keep the stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lyvrj5Agw0f3",
        "outputId": "7e242d3b-6762-40be-efe4-b1aa866db57d"
      },
      "outputs": [],
      "source": [
        "teacher_df = utterances[utterances['speaker'] == 'teacher'].sample(NUM_SAMPLES)\n",
        "student_df = utterances[utterances['speaker'] == 'student'].sample(NUM_SAMPLES)\n",
        "teacher_df[CLEAN_TEXT_COLUMN] = teacher_df[TEXT_COLUMN].apply(lambda x: clean_text_to_words(x, keep_stopwords=False, stem=False))\n",
        "student_df[CLEAN_TEXT_COLUMN] = student_df[TEXT_COLUMN].apply(lambda x: clean_text_to_words(x, keep_stopwords=False, stem=False))\n",
        "plot_log_odds(teacher_df, student_df, logodds_factor=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf9Obxzd5b1B"
      },
      "source": [
        "As you can see, the log-odds analysis changes depending on how we process the text. It's important to take your use case into account when making these decisions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3aO1XmoYw0f5"
      },
      "source": [
        "#### Topic Modeling\n",
        "\n",
        "Next, let's try out topoic modeling on the data. For now, we'll just use the teacher's utterances.\n",
        "\n",
        "\n",
        "Mallet for colab: https://colab.research.google.com/github/aiforpeople-git/First-AI4People-Workshop/blob/master/NLP_AI/Topic_Modeling.ipynb#scrollTo=fgt9WNMgNvqQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vltraX77w0f6",
        "outputId": "c514f6e7-3000-43cb-e509-addfb49184a0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gensim\n",
        "\n",
        "import os\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version\n",
        "install_java()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_tEgkRvw0f7",
        "outputId": "a33b2f8d-0e58-4733-f1aa-4f28bb483180"
      },
      "outputs": [],
      "source": [
        "!pip install little_mallet_wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1EvNq12w0f7"
      },
      "outputs": [],
      "source": [
        "# Following the example in https://github.com/maria-antoniak/little-mallet-wrapper/blob/master/demo.ipynb\n",
        "\n",
        "import little_mallet_wrapper as lmw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LihsGryUw0f7",
        "outputId": "b5b256e4-46f0-4528-c2dc-520a21b0d5d8"
      },
      "outputs": [],
      "source": [
        "path_to_mallet = os.path.join(DIR_PATH, 'mallet-2.0.8/bin/mallet')\n",
        "print(path_to_mallet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcYdRvAkw0f7",
        "outputId": "2b04c0d1-ee07-4da9-94ff-9586ec4f03b2"
      },
      "outputs": [],
      "source": [
        "training_data = [lmw.process_string(text) for text in teacher_df[TEXT_COLUMN].tolist()]\n",
        "training_data = [d for d in training_data if d.strip()]\n",
        "\n",
        "print(f\"Number of training documents: {len(training_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ34Fmiuw0f7",
        "outputId": "77e16624-6fa3-4a39-a597-87296623fdc8"
      },
      "outputs": [],
      "source": [
        "# Training a topic model\n",
        "\n",
        "NUM_TOPICS = 5\n",
        "OUTPUT_DIR = os.path.join(DIR_PATH, 'output')\n",
        "\n",
        "topic_keys, topic_distributions = lmw.quick_train_topic_model(\n",
        "    path_to_mallet, OUTPUT_DIR, num_topics=NUM_TOPICS, training_data=training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLj38JU5w0f7",
        "outputId": "9456c5a4-da4f-41bd-d676-3ddb825901c1"
      },
      "outputs": [],
      "source": [
        "for i, t in enumerate(topic_keys):\n",
        "    print(i, '\\t', ' '.join(t[:10]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rd21Z3xmw0f7"
      },
      "source": [
        "#### Clustering\n",
        "\n",
        "\n",
        "Another class of unsupervised methods that can be performed on the _sentence-level_ are clustering methods.\n",
        "Note that before, we ran unsupervised methods on the _word-level_. Different units of analysis can yield different insights, depending on what we might be interested in finding!\n",
        "\n",
        "Again, we will run this analysis on the sample of teacher's utterances.\n",
        "\n",
        "An extremely useful library is https://www.sbert.net/. We will be using this in the following sections.\n",
        "\n",
        "You may find the complete resource on clustering here: https://www.sbert.net/examples/applications/clustering/README.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mllm5UZFw0f8",
        "outputId": "89612519-9162-4383-f82b-6ccfb4fa5629"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U-9VD7Ow0f8",
        "outputId": "16779958-f88c-4b6b-e612-1473988ab363"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "NUM_CLUSTERS=10\n",
        "\n",
        "sentences = teacher_df[TEXT_COLUMN].tolist()\n",
        "\n",
        "# We are going to be using Agglomerative Clustering to cluster the sentences\n",
        "\n",
        "corpus_embeddings = model.encode(sentences)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Perform kmean clustering\n",
        "clustering_model = AgglomerativeClustering(distance_threshold=1.5, n_clusters=None)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(sentences[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIW0t1sTw0f8"
      },
      "source": [
        "## Assignments\n",
        "\n",
        "Now it's your turn to analyze the data!\n",
        "\n",
        "### 1: Compute top 10 words for the student.\n",
        "\n",
        "Your task is to plot a histogram of the top 10 words (alphabetic strings, non-stop words) uttered by the student in the entire transcript dataset.\n",
        "The x-axis should be the words, and the y-axis should be the number of occurences.\n",
        "\n",
        "**IMPORTANT FOR GRADING**:\n",
        "- Complete the code for computing the top 10 words.\n",
        "- Plot the results when you upload the PDF of the colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbblOyv4w0f8"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CaANf7TRw0f9"
      },
      "source": [
        "### 2: Plot student statistics\n",
        "\n",
        "Use the documentation in the metadata to compute and report the following:\n",
        "\n",
        "- Number of students\n",
        "- % Male\n",
        "- % Black\n",
        "- % Asian\n",
        "- % Hispanic/Asian\n",
        "- % White\n",
        "- % Free/Reduced Lunch\n",
        "- % Special Education status\n",
        "- % Limited English proficiency\n",
        "\n",
        "\n",
        "**IMPORTANT FOR GRADING:**\n",
        "- Complete the code for computing the student statistics.\n",
        "- Have your answers for the following printed out when you upload the PDF version of the colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVTe_rL_w0f9",
        "outputId": "7c083c0d-18be-4a32-f428-c7c2e080d467"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "num_students = None # REPLACE ME!\n",
        "print(f\"Number of students = {num_students}\")\n",
        "\n",
        "perc_male = None # REPLACE ME!\n",
        "print(f\"Percentage male = {perc_male}\")\n",
        "\n",
        "perc_black = None # REPLACE ME!\n",
        "print(f\"Percentage black = {perc_black}\")\n",
        "\n",
        "perc_asian = None # REPLACE ME!\n",
        "print(f\"Percentage asian = {perc_asian}\")\n",
        "\n",
        "perc_hispanic = None # REPLACE ME!\n",
        "print(f\"Percentage hispanic = {perc_hispanic}\")\n",
        "\n",
        "perc_white = None # REPLACE ME!\n",
        "print(f\"Percentage white = {perc_white}\")\n",
        "\n",
        "perc_free_lunch = None # REPLACE ME!\n",
        "print(f\"Percentage free lunch = {perc_free_lunch}\")\n",
        "\n",
        "perc_special_ed = None # REPLACE ME!\n",
        "print(f\"Percentage special ed = {perc_special_ed}\")\n",
        "\n",
        "perc_limited_english = None # REPLACE ME!\n",
        "print(f\"Percentage limited english = {perc_limited_english}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VxbgbgvCw0f9"
      },
      "source": [
        "#### Log odds of transcripts from teachers with highest vs. lowest MQI5 scores\n",
        "\n",
        "This question will involve multiple steps.\n",
        "\n",
        "1. Identify the metadata corresponding to the MQI5 scores.\n",
        "2. Load the corresponding metadata file.\n",
        "3. Identify the top 10 and bottom 10 transcript IDs with the highest and lowest MQI5 scores.\n",
        "4. Load their transcripts and run the log-odds analysis on the teacher's word level.\n",
        "5. Write up a short analysis and summary of your observations.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TaG0-ULdw0f-"
      },
      "source": [
        "##### Step 1: Identify the metadata corresponding to the value-added scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR ANSWER HERE (PUT IN THE DIRECTORY AND NAME OF THE OUTCOME COLUMN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7xjrU85w0f-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AG4cODidw0f-"
      },
      "source": [
        "##### Step 2: Load the metadata file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZNCsqu1w0f_"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9HAXbUlFw0f_"
      },
      "source": [
        "##### Step 3: Identify the top 5 and bottom 5 teacher IDs with the highest and lowest value-added scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWTm0Zauw0f_"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RBtfq-Rnw0f_"
      },
      "source": [
        "##### Step 4: Load their transcripts and run the log-odds analysis on the teacher's word level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S7AMx8qw0f_"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ASTJbDdNw0f_"
      },
      "source": [
        "##### Step 5: Write up a short analysis and summary of your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdeOHCnuw0f_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "YOUR ANALYSIS AND OBSERVATIONS HERE\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WAwuuOe4w0f_"
      },
      "source": [
        "## Extra Assignments\n",
        "\n",
        "You have the choice to do extra credit work on this assignment. Below are some ideas of what you can do. If you have other ideas, make sure you check with Rose and/or Dora that it's reasonable to do.\n",
        "\n",
        "1. Play around with an unsupervised algorithm (e.g., clustering) to identify different pedagogy categories. Keep a log of things that you try out, things that do and don't work.\n",
        "\n",
        "2. Perform the same log-odds analysis except with value-added scores as the outcome measure. What do you observe?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
